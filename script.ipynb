{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install pandas \n",
    "!pip install spacy \n",
    "!pip install unicodedata \n",
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import unicodedata\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "import pickle\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_df(df):\n",
    "    translator = Translator()\n",
    "    D = {\n",
    "        'Job_offer':[],\n",
    "        'Label':[],\n",
    "        'Traslation':[]\n",
    "    }\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        try:\n",
    "            traslation = translator.translate(str(row['Job_offer']), dest='en').text\n",
    "        except:\n",
    "            continue\n",
    "        D['Job_offer'].append(row['Job_offer'])\n",
    "        D['Label'].append(row['Label'])\n",
    "        D['Traslation'].append(traslation)\n",
    "    new_df = pd.DataFrame.from_dict(D)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm',  disable=['parser', 'ner'])\n",
    "\n",
    "def add_token_col(df):\n",
    "    D = {\n",
    "        'Job_offer':[],\n",
    "        'Label':[],\n",
    "        'Traslation':[],\n",
    "        'token':[]\n",
    "    }\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        tokens = None\n",
    "        message = row['Traslation']\n",
    "        # Get lemma\n",
    "        tokens = [token.lemma_ for token in sp(message)]\n",
    "\n",
    "        # Normalize Unicode String and convert to lowercase\n",
    "        tokens = [unicodedata.normalize('NFKD', token).lower() for token in tokens]\n",
    "\n",
    "        #print('Removing all but chars and numbers...')\n",
    "        tokens = [re.sub(r'[\\W_]+', '',token) for token in tokens] \n",
    "\n",
    "        # Remove numbers, but not words that contain numbers.\n",
    "        tokens = [token for token in tokens if not token.isnumeric()]\n",
    "\n",
    "        # Remove words that are only one or two characters.\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "\n",
    "        # Remove stopwords \n",
    "        stop_words = stopwords.words('english')\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Strip punctuation\n",
    "        tokens = [strip_punctuation(token) for token in tokens]\n",
    "        D['Job_offer'].append(row['Job_offer'])\n",
    "        D['Label'].append(row['Label'])\n",
    "        D['Traslation'].append(row['Traslation'])\n",
    "        D['token'].append(' '.join(tokens))\n",
    "    new_df = pd.DataFrame.from_dict(D)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag:\n",
    "    train_df = pd.read_csv('train_set.csv')\n",
    "    test_df = pd.read_csv('test_set.csv')\n",
    "    new_train_df = get_new_df(train_df)\n",
    "    new_train_df.to_csv('new_train_set.csv', index=False)\n",
    "    new_test_df = get_new_df(test_df)\n",
    "    new_test_df.to_csv('new_test_set.csv', index=False)\n",
    "    new_train_df = add_token_col(new_train_df)\n",
    "    new_train_df.to_csv('new_train_set.csv', index=False)\n",
    "    new_test_df = add_token_col(new_test_df)\n",
    "    new_test_df.to_csv('new_test_set.csv', index=False)\n",
    "else:\n",
    "    new_train_df = pd.read_csv('new_train_set.csv')\n",
    "    new_test_df = pd.read_csv('new_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score, recall_score,roc_curve,roc_auc_score\n",
    "\n",
    "\n",
    "# Creare un oggetto CountVectorizer per trasformare i token in vettori numerici\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# Creare un oggetto Random Forest Classifier per la classificazione\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "\n",
    "# Creare un oggetto Multi-Output Classifier e passare il classificatore Random Forest come parametro\n",
    "multi_target = MultiOutputClassifier(rf)\n",
    "\n",
    "\n",
    "# Creare un pipeline con il CountVectorizer e il Multi-Output Classifier\n",
    "RandomForest = Pipeline([\n",
    "   ('vectorizer', vectorizer),\n",
    "   ('multi_target', multi_target)\n",
    "])\n",
    "\n",
    "# Addestrare il modello sui dati di addestramento (assumendo che il DataFrame di input sia chiamato \"df\" e le colonne delle etichette di output siano chiamate \"label1\" e \"label2\")\n",
    "RandomForest.fit(new_train_df['token'], new_train_df[['Label']])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, new_test_df):\n",
    "    predictions = model.predict(new_test_df['token'])\n",
    "    r1=recall_score(predictions, new_test_df['Label'], average='weighted')\n",
    "    print('recall_score', r1)\n",
    "    p1=precision_score(predictions, new_test_df['Label'], average='weighted')\n",
    "    print('precision_score', p1)\n",
    "    f1 = f1_score(predictions, new_test_df['Label'], average='weighted')\n",
    "    print('f1_score', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score, recall_score,roc_curve,roc_auc_score\n",
    "\n",
    "\n",
    "# Creare un oggetto CountVectorizer per trasformare i token in vettori numerici\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "# Creare un oggetto Random Forest Classifier per la classificazione\n",
    "rf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                   hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "\n",
    "# Creare un oggetto Multi-Output Classifier e passare il classificatore Random Forest come parametro\n",
    "multi_target = MultiOutputClassifier(rf)\n",
    "\n",
    "\n",
    "# Creare un pipeline con il CountVectorizer e il Multi-Output Classifier\n",
    "MLP = Pipeline([\n",
    "   ('vectorizer', vectorizer),\n",
    "   ('multi_target', multi_target)\n",
    "])\n",
    "MLP.fit(new_train_df['token'], new_train_df[['Label']])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(RandomForest, new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(MLP, new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(RandomForest, open('RND.pkl', 'wb'))\n",
    "pickle.dump(MLP, open('MLP.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df = get_new_df(df)\n",
    "df = add_token_col(df)\n",
    "RND = pickle.load(open('RND.pkl', 'rb'))\n",
    "MLP = pickle.load(open('MLP.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(RND, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(MLP, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
